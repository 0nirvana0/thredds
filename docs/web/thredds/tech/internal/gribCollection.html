<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Untitled Document</title>
</head>

<body>
<h2>Grib Collection Implementation Notes</h2>
<p>6/15/2012</p>
<h3>Index</h3>
<p>Example 1. GFS global half degree - 45 day archive</p>
<pre>
 Rectilyser: nvars=117 records unique=132684 total=3637805 dups=3505121 (0.963526)
 createIndex for /data/ldm/pub/native/grid/NCEP/GFS/Global_0p5deg/NCEP-GFS-Global_0p5deg.ncx
 write RecordMaps: bytes = 1265209 record = 132684 bytesPerRecord=9
 write GribCollectionIndex= 35929 bytes</pre>
<p>There are a total of 3.6M grib records, but only 132K are unique and used in the collection dataset. This is typical when making a collection of forecast model runs, where the forecast times heavily overlap.</p>
<p>When opening the collection dataset, only 36K has to be read in. The remaining bytes of the 1.26M collection index are the record lookup section, and are read in only when a variable's data is requested.</p>
<p>In this example, computing the collection index takes around 60 sec,  with 4Gb heap space, but 30 minutes with 2Gb heap. That probaby means that the index took just under 2Gb heap and was doing excessive GC. So one needs large heap sizes to compute these large collections.</p>
<h3>Assumptions on the GRIB encoding</h3>
<p>The collection of GRIB records is not arbitrary, but must be coherent (eg come all from the same model) such that the following assumptions are valid:</p>
<ol>
  <li>All Grib records are assumed to have the same center, subcenter, and master and local table versions. This is used for determiing which GRIB code and template tables to use.</li>
  <li>Grib records can be distributed arbitrarily among the collection of files.</li>
  <li>Unique variables are created by hashing the<em> GDS, PDS template, discipline, category, parameter, </em> <em>level type</em>, and <em>level layer flag</em>. Also, if they apply, the<em> statistical process type</em> (code table 4.10), and the <em>ensemble derived type</em>  (table 4.7).</li>
  <li>The collection of GRIB records for each unique variable is turned into a multidimensional grid, by taking the cartesion product of<strong><em> time X ens X vert </em></strong>coordinates. Where there are missing records, the library will return missing data. Where there are duplicate records, later records replace earlier records, where later means later in one file, or in a later file where the file collection is sorted lexigraphically.</li>
  <li>If there are multiple GDS (Grid Definition Section) in the collection, each unique GDS and the variables that use it becomes a seperate <em>Group</em>. GDS uniqueness is based on a hashcode. Unfortunately, there may be roundoff errors and/or minor variations in GDS encodings. The CDM tries to allow for this in <em>Grib2Gds.hashcode()</em>. If you see variables unexpectedly split into different groups with apparently the same projection, likely the hashcode for that projection type needs to be modified.</li>
  <li>All of the  time, ensemble, and vertical coordinates for each variable in one group are compared, and where they are identical, are shared between variables.</li>
</ol>
<h3>RAF Caching</h3>
<pre>    // GribCollection : default is allow 100 - 200 open files, cleanup every 15 minutes<br />    min = ThreddsConfig.getInt(&quot;GribCollection.minFiles&quot;, 100);<br />    max = ThreddsConfig.getInt(&quot;GribCollection.maxFiles&quot;, 200);<br />    secs = ThreddsConfig.getSeconds(&quot;GribCollection.scour&quot;, 15 * 60);<br />    if (max &gt; 0) {<br />      GribCollection.initFileCache(min, max, secs);<br />      startupLog.info(&quot;CdmInit: GribCollection.initFileCache= [&quot;+min+&quot;,&quot;+max+&quot;] scour = &quot;+secs);<br />    }</pre>
<p>&nbsp;</p>
<h2>Time Partitions</h2>
<p>&nbsp;</p>
<pre>// date matcher part of spec - #date# - can only be in name
// time partition by directory

&lt;collection spec=&quot;G:/nomads/cfsr/timeseries/**/.*grb2$&quot; dateFormatMark=&quot;#timeseries/#yyyyMM&quot; name=&quot;CFSR-timeseries&quot; timePartition=&quot;directory&quot; /&gt;

      
// dateFormatMark in seperate attribute - #match literal# - works on the path
// time partition by day
        &lt;collection<br />            spec=&quot;/data/ldm/pub/native/grid/NCEP/SREF/CONUS_40km/ensprod_biasc/.*grib2$&quot;<br />            name=&quot;SREF_CONUS_40km_ensprod_biasc&quot;<br />            dateFormatMark=&quot;#SREF_CONUS_40km_ensprod_biasc_#yyyyMMdd_HHmm&quot;<br />            timePartition=&quot;day&quot;<br />            olderThan=&quot;5 min&quot;/&gt;






// date matcher part of spec - #date# - can only be in name

  &lt;collection<br />            spec=&quot;/data/ldm/pub/native/grid/NCEP/SREF/CONUS_40km/pgrb_biasc/SREF_CONUS_40km_pgrb_biasc_[a-z]*_#yyyyMMdd_HHmm#.grib2$&quot;<br />            name=&quot;SREF_CONUS_40km_ensprod_biasc&quot;<br />            timePartition=&quot;day&quot;<br />            olderThan=&quot;5 min&quot;/&gt;

        &lt;collection<br />            spec=&quot;/data/ldm/pub/native/grid/NCEP/SREF/CONUS_40km/pgrb_biasc/.*grib2$&quot;<br />            name=&quot;SREF_CONUS_40km_ensprod_biasc&quot;<br />            dateFormatMark=&quot;yyyyMMdd_HHmm#.grib2#&quot;<br />            timePartition=&quot;day&quot;<br />            olderThan=&quot;5 min&quot;/&gt;<br />
 </pre>
<p>&nbsp;</p>
<p>cannot mix ensembles and vertical levels across the partition.</p>
<p> for each variable, create union of times: for each time track (value, partition, localIndex). merge time coords with same {value}. store {partition, localIndex} cmmon case is that variables will share  {partition, localIndex}, but is it worth merging?</p>
<p>dont need proto change; its reselected when index is rebuilt</p>
<h2>check for changes</h2>
<p>handled by DCM, looking only at underlying collection.</p>
<h3>GribCollection</h3>
<ol>
  <li>look for new or deleted files</li>
  <li>look if newFile.lastModified()  &gt; oldFile.lastModified() </li>
  <li>look if index (gbx9) file doesnt exist, or has lastModified date   &lt; oldFile.lastModified() </li>
</ol>
<p>&nbsp;</p>
<p>cant examine all files to see whats changed - too mant files. only examine each partition. maybe examine the latest?? maybe not - use manual rescan ??</p>
</body>
</html>
